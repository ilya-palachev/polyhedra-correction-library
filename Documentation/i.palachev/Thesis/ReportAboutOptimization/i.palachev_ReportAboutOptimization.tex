
% Copyright (c) 2009-20134 Ilya Palachev <iliyapalachev@gmail.com>
% This document contains the report about reconstruction of convex sets.

% Declare the class of document: size of paper, size of font, and etc.
% Type of the document is "article".
\documentclass[a4paper, 12pt, titlepage]{article} 

% Package that enables setting the size of free spaces at the border of the 
% page with the command  \geometry :
\usepackage{geometry}
\geometry {
   left=3cm,
   right=1.5cm,
   top=2cm,
   bottom=2cm
}

\usepackage[utf8]{inputenc}

\usepackage[english,russian]{babel}

\usepackage{amsmath}

%\usepackage{cmap}

\usepackage{indentfirst}

\usepackage{a4wide,amssymb}

%\usepackage[pdftex]{graphicx}

%\usepackage{wrapfig}

%\linespread{1.3}               % полтора интервала. Если 1.6, то два интервала
\pagestyle{plain}               % номерует страницы

\usepackage{graphicx}
\renewcommand{\topfraction}{1}
\renewcommand{\textfraction}{0}

% Package that enables usage of theorems and definition designed in a 
% standard way.
% http://en.wikibooks.org/wiki/LaTeX/Theorems
\usepackage{amsthm}

% Create environment for smart definitions
\theoremstyle{definition}
\newtheorem{SmartDefinition}{Определение}

% Create environment for smart theorems
\theoremstyle{plain}
\newtheorem{SmartTheorem}{Теорема}

% Create environment for smart lemmas
\theoremstyle{plain}
\newtheorem{SmartLemma}{Лемма}

% The following code enables back references:
\usepackage{color} 
\definecolor{darkgreen}{rgb}{0,.5,0} 
\usepackage[unicode,colorlinks,filecolor=blue,citecolor=darkgreen,pagebackref]
{hyperref}

% The package that provides symbols like \Square:
\usepackage{wasysym}

%opening
\title{Отчет по методам оптимизации}
\author{Палачев Илья}

\begin{document}

\maketitle

\tableofcontents

\newpage
\section{Линейное программирование}
Одним из самых распространенных классов задач оптимизации являются задачи
\textbf{линейного программирования}. Методы решения этих задач имеют широкое
приложение в разнообразных прикладных областях науки. Самым распространенным
методом решения задач линейного программирования является так называемый
\textbf{симплекс-метод}, который будет далее описан в этом разделе.

\textit{Общая задача линейного программирования} формулируется следующим
образом.

\begin{equation}
\label{eq:gen-lp}
 J(u) = c_{1} u^{1} + \ldots + c_{n} u^{n} \to min
\end{equation}

при условиях

\begin{equation}
\label{eq:gen-lp-cond1}
 u^{k} \geq 0, \;\;\;\;\; k \in I
\end{equation}

\begin{equation}
\label{eq:gen-lp-cond2}
 \left\{
  \begin{aligned}
   a_{11} u^{1} + \ldots + a_{1n} u^{n} \leq 0 \\
   \ldots \\
   a_{m1} u^{1} + \ldots + a_{mn} u^{n} \leq 0 \\
  \end{aligned}
 \right.
\end{equation}

\begin{equation}
\label{eq:gen-lp-cond3}
 \left\{
  \begin{aligned}
   a_{m + 1, 1} u^{1} + \ldots + a_{m + 1, n} u^{n} = 0 \\
   \ldots \\
   a_{s1} u^{1} + \ldots + a_{sn} u^{n} = 0 \\
  \end{aligned}
 \right.
\end{equation}

где $I \subset \{1, 2, \ldots, n\}$ - множество индексов переменных, которые
требуется поддерживать положительными.

В векторном виде задачу (\ref{eq:gen-lp}), (\ref{eq:gen-lp-cond1}),
(\ref{eq:gen-lp-cond2}), (\ref{eq:gen-lp-cond3}) можно записать следующим
образом:

\begin{equation}
 \begin{aligned}
  J(u) = \langle c, u \rangle \to min \\
  u \in U = \{u \in \mathbb{R}^{n} \;\; | \;\; u^{k} \geq 0, k \in I,
  A u \leq 0, \overline{A} u \overline{b}\}
 \end{aligned}
\end{equation}

В качестве особого подкласса общей задачи линейного программирования
выделяют так называемую \textit{каноническую задачу}:

\begin{equation}
 \begin{aligned}
  J(u) = \langle c, u \rangle \to min \\
  u \in U = \{u \in \mathbb{R}^{n} \;\; | \;\; u \geq 0,
  \overline{A} u = \overline{b}\}
 \end{aligned}
\end{equation}

а также \textit{основную задачу} линейного программирования:

\begin{equation}
 \begin{aligned}
  J(u) = \langle c, u \rangle \to min \\
  u \in U = \{u \in \mathbb{R}^{n} \;\; | \;\; u \geq 0,
  \overline{A} u \leq \overline{b}\}
 \end{aligned}
\end{equation}

Можно показать, что каждую из трех задач можно свести к любой другой с помощью
введения дополнительных переменных. На практике как правило к этому стараются
не прибегать, поскольку рост числа переменных обходится дорого с вычислительной 
точки зрения. Далее мы будем рассматривать методы решения канонической задачи.

Из геометрических соображений видно, что множество ограничений $U$ является
многогранным, причем либо ограниченным, либо неограниченным. Если рассмотреть
поверхности уровня минимизируемого функционала, которые являются аффинными
подпространствами в $\mathbb{R}^{n}$, то пересекая их с множеством $U$, мы
сможем выявить поверхность самого низкого уровня. На ней и будет достигаться
минимум функционала. Это поверхность будет являться опорной к множеству
ограничений, поэтому в ее пересечении с $U$ будет содержаться хотя бы одна 
вершина $U$. В теории линейного программирования такие вершины принято
называть \textit{угловыми}.

\begin{SmartDefinition}
 Пусть $U$ -- выпуклое множество в $\mathbb{R}^{n}$. Тогда $v \in U$ называется
 \textbf{угловой точкой} множества $U$, если представление
 $v = \alpha v_{1} + (1 - \alpha) v_{2}$ при $v_{1}, v_{2} \in U$ и
 $0 < \alpha < 1$ возможно лишь при $v_{1} = v_{2}$. Иначе говоря $v$ -- угловая
 точка множества $U$, если она не является внутренней точкой никакого отрезка,
 принадлежащего множеству $U$.
\end{SmartDefinition}

В случае канонической задачи множество $U$ представляет из себя следующее:

\begin{equation}
\label{eq:canon-cond}
 U = \{u \in \mathbb{R}^{n} \;\; | \;\; u \geq 0, A u = b\}
\end{equation}

Критерий того, что точка является угловой точкой множества определяет следующая
теорема.

\begin{SmartTheorem}
 Пусть множество определяется условиями (\ref{eq:canon-cond}), $A \neq 0$,
 $r = rang A$ -- ранг матрицы $A$. Для того, чтобы точка
 $v = (v_{1}, v_{2}, \ldots, v_{n})$ была угловой точкой множества $U$,
 необходимо и достаточно, чтобы существовали набор из $r$ номеров
 $j_{1}, \ldots, j_{r}$ ($1 \leq j_{l} \leq n, l = 1, \ldots, r$) таких, что
 \begin{equation}
 \label{eq:vertex-def}
  A_{j_{1}} v^{j_{1}} + \ldots + A_{j_{r}} v^{j_{r}} = b;
  \;\; v^{j} = 0, j \neq j_{l}, l = 1, \ldots, r
 \end{equation}
\end{SmartTheorem}

В последнем определении существенно, что в (\ref{eq:vertex-def}) берутся ровно
$r$ векторов $A_{j_{1}}, \ldots, A_{j_{r}}$ -- ровно такое число векторов,
каков ранг матрицы $A$.

\begin{SmartDefinition}
 Систему векторов $A_{j_{1}}, \ldots, A_{j_{r}}$, входящих в первое из равенств
 (\ref{eq:vertex-def}), называют \textit{базисом угловой точки} $v$, а
 соответствующие им величины $v^{j_{1}}, \ldots, v^{j_{r}}$ --
 \textit{базисными координатами} угловой точки $v$. Если все базисные 
 координаты угловой точки положительны, то такую угловую точку называют
 \textit{невырожденной}. В противном случае -- \textit{вырожденной}. При
 фиксированном базисе $A_{j_{1}}, \ldots, A_{j_{r}}$ переменные
 $u^{j_{1}}, \ldots, u^{j_{r}}$ называются \textit{базисными переменными}
 угловой точки, а остальные $u^{j}$ -- \textit{небазисными (свободными)
 переменными}.
\end{SmartDefinition}

Для того чтобы избежать трудоемкого полного перебора всех угловых точек
множества ограничений $U$, выбирают одну, от нее переходят к другой, у которой
значение функционала меньше и т. д. В этом и состоит симплекс-метод.

Далее будем считать, что $m = n$ и все уравнения ограничений, линейно зависимые
от других, выброшены.

Пусть нам известна угловая точка $v$. Пусть $\overline{u} = (u_{1},
u_{r})$ -- вектор, составленный из базисных переменных угловой точки
$v$. Перепишем систему ограничений в следующем виде:

\begin{equation}
 B \overline{u} + A_{r + 1} u^{r + 1} + \ldots + A_{n} u^{n} = b
\end{equation}

Домножим эту систему слева на $B^{-1}$, базисные переменные через
небазисные и получим следующее:

\begin{equation}
\label{eq:nonfree-via-free}
 \begin{aligned}
  u^{1} = v^{1} - \gamma_{1, r + 1} u^{r + 1} - \ldots - \gamma_{1 k} u^{k} -
  \ldots - \gamma_{1 n} u^{n} \\
  \ldots \\
  u^{i} = v^{i} - \gamma_{i, r + 1} u^{r + 1} - \ldots - \gamma_{i k} u^{k} -
  \ldots - \gamma_{i n} u^{n} \\
  \ldots \\
  u^{r} = v^{r} - \gamma_{r, r + 1} u^{r + 1} - \ldots - \gamma_{r k} u^{k} -
  \ldots - \gamma_{r n} u^{n} \\
 \end{aligned}
\end{equation}

Подставив эти выражения в формулу функционала, получим:

\begin{equation}
 J(u) = J(v) - \sum \limits_{i = r + 1}^{n} \Delta_{i} u^{i}
\end{equation}

где

\begin{equation}
 \Delta_{i} = \langle \overline{c}, B^{-1} A_{i} \rangle =
 \sum \limits_{s = 1}^{r} c_{s} \gamma_{s i} - c_{i}
\end{equation}

Запишем величины $\gamma_{s k}, v^{i}, \Delta_{i}$ в так называемую
симплекс-таблицу:

\begin{table}[hh]
\caption{Симплекс-таблица}
\label{simplex-table}
\begin{center}
\begin{tabular}{|p{2cm}|c|c|c|c|c|c|c|c|c|c|c|c|c|c|p{2cm}|}
\hline
Базисные \par переменные & $u^{1}$ & $\ldots$ & $u^{i}$ & $\ldots$ &
$u^{s}$ & $\ldots$ & $u^{r}$ & $u^{r + 1}$ & $\ldots$ & $u^{k}$ & $\ldots$ &
$u^{j}$ & $\ldots$ & $u^{n}$ & Свободные \par члены \\
\hline
$u^{1}$ & 1 & $\ldots$ & 0 & $\ldots$ & 0 & $\ldots$ & 0 & $\gamma_{1, r + 1}$ &
$\ldots$ & $\gamma_{1k}$ & $\ldots$ & $\gamma_{1j}$ & $\ldots$ & $\gamma_{1n}$
& $v^{1}$ \\
$\ldots$ & $\ldots$ & $\ldots$ & $\ldots$ & $\ldots$ & $\ldots$ & $\ldots$ &
$\ldots$ & $\ldots$ & $\ldots$ & $\ldots$ & $\ldots$ & $\ldots$ & $\ldots$ &
$\ldots$ & $\ldots$ \\
$u^{i}$ & 0 & $\ldots$ & 1 & $\ldots$ & 0 & $\ldots$ & 0 & $\gamma_{i, r + 1}$ &
$\ldots$ & $\gamma_{i k}$ & $\ldots$ & $\gamma_{i j}$ & $\ldots$ &
$\gamma_{i n}$ & $v^{i}$ \\
$\ldots$ & $\ldots$ & $\ldots$ & $\ldots$ & $\ldots$ & $\ldots$ & $\ldots$ &
$\ldots$ & $\ldots$ & $\ldots$ & $\ldots$ & $\ldots$ & $\ldots$ & $\ldots$ &
$\ldots$ & $\ldots$ \\
$u^{s}$ & 0 & $\ldots$ & 0 & $\ldots$ & 1 & $\ldots$ & 0 & $\gamma_{s, r + 1}$ &
$\ldots$ & $\gamma_{s k}$ & $\ldots$ & $\gamma_{s j}$ & $\ldots$ &
$\gamma_{s n}$ & $v^{s}$ \\
$\ldots$ & $\ldots$ & $\ldots$ & $\ldots$ & $\ldots$ & $\ldots$ & $\ldots$ &
$\ldots$ & $\ldots$ & $\ldots$ & $\ldots$ & $\ldots$ & $\ldots$ & $\ldots$ &
$\ldots$ & $\ldots$ \\
$u^{r}$ & 0 & $\ldots$ & 0 & $\ldots$ & 0 & $\ldots$ & 1 & $\gamma_{r, r + 1}$ &
$\ldots$ & $\gamma_{r k}$ & $\ldots$ & $\gamma_{r j}$ & $\ldots$ &
$\gamma_{r n}$ & $v^{r}$ \\
\hline
Функция & 0 & $\ldots$ & 0 & $\ldots$ & 0 & $\ldots$ & 0 & $\Delta_{r + 1}$ &
$\ldots$ & $\Delta_{k}$ & $\ldots$ & $\Delta_{j}$ & $\ldots$ & $\Delta_{n}$ &
$J(v)$ \\
\hline
\end{tabular}
\end{center}
\end{table}

Опишем коротко алгоритм линейного программирования. Допустим, что множество $U$
непусто и нам уже известна некоторая угловая точка $v$.

Будем менять по очереди небазисные переменные, тогда функционал будет меняться
по формуле

\begin{equation}
 J(u) = J(v) - \Delta_{k} u^{k}
\end{equation}

Будем искать такой номер $k$ ($r + 1 \leq k \leq n$) и такую величину $u^{k}$,
чтобы точка $w$, полученная по формулам (\ref{eq:nonfree-via-free}) для набора
свободных переменных $0, \ldots, 0, u_{k}, 0, \ldots, 0$, удовлетворяла
условиям связи $A w = b$. В зависимости от знаков величин $\gamma_{s k},
\Delta_{i}$ выделяют три возможных случая:

\textbf{Случай 1.} $\Delta_{i} \leq 0$ для всех $i = r + 1, \ldots, n$. Тогда 
можно показать, что $v$ -- точка минимума функционала.

\textbf{Случай 2.} Существует такой номер $k$ ($r + 1 \leq k \leq n$), что 
$\Delta_{k} > 0$, но все $\gamma_{i k} \leq 0$ для всех $i = 1, \ldots, r$.
Тогда можно показать, что функционал не является ограниченным снизу на $U$.

\textbf{Случай 3.} Существуют номера $k$ ($r + 1 \leq k \leq n$),
$i$ ($1 \leq i \leq r$) такие что $\Delta_{k} > 0$ и $\gamma_{i k} > 0$.
Для того чтобы обеспечить выполнение условий связи полагают
$u^{k} = v_{s} / \gamma_{s k}$, где индекс $s$ выбран следующим образом:

\begin{equation}
 \operatornamewithlimits{min}_{i \in I_{k}} v^{i} / \gamma_{i k} =
 v_{s} / \gamma_{s k}
\end{equation}

Соответствующую величину $\gamma_{s k}$ называют \textit{разрешающим элементом}
симплекс-таблицы.

Можно показать что полученная таким образом точка $w$ является угловой, 
реализует меньшее значение функционала, чем точка $v$, и что вектора
$A_{1}, \ldots, A_{s - 1}, A_{s + 1}, \ldots, A_{r}, A_{k}$ образуют ее базис.
После этого пересчитывают величины симплекс-таблицы по тем же принципам,
которые описаны выше, и вновь ищут следующую угловую точку.

Описанная выше процедура работает для случая, когда точка $v$ является
невырожденной. Если же она вырождена, то во время работы алгоритма может
произойти \textit{зацикливание}. Для того, чтобы его избежать, применяют
специальный метод, называемый \textbf{антициклином}. Он состоит в том, что
симплекс-таблица расширяется дополнительными столбцами единичной матрицы
порядка $r \times r$. На каждом шаге эти столбцы преобразуются по тем же
правилам, что и обычные. В случае вырожденной угловой точки эти столбцы
позволяют выбрать такой особый базис угловой точки $v$ (которых, вообще говоря,
должно быть несколько), что зацикливания можно избежать.

Указанные выше построения основывались на том, что некоторая угловая точка
множества $U$ уже известна. Для того, чтобы найти первую угловую точку также
применяют особый метод.

А именно, множество переменных $u = (u^{1}, \ldots, u^{n})$ расширяется
дополнительными переменными $w = (u^{n + 1}, \ldots, u^{n + m})$ и в
пространстве $\mathbb{R}^{n + m}$ относительно переменных
$z = (u, w)^{T}$ рассматривают следующую каноническую задачу линейного
программирования:

\begin{equation}
\begin{aligned}
 J_{1}(z) = u^{n + 1} + \ldots + u^{n + m} \to min \\
 z \in Z = \{ z \in \mathbb{R}^{n + m} | z \geq 0,
 C z \equiv A u + w = b \} \\
\end{aligned}
\end{equation}

Оказывается, что для нее точка $z_{0} = (0, b) \geq 0$ является угловой точкой
множества $Z$. Очевидно, что $J_{1}(z) \geq 0$ при всех $z \in Z$. Поэтому
алгоритм симлекс-метода обязательно сходится. Пусть он сойдется к некоторой
точке $z_{*}$. Тогда можно показать, что если $J_{1}(z_{*}) > 0$, то
$U = \varnothing$, а если $J_{1}(z_{*}) = v^{n + 1} + \ldots + v^{n + m} = 0$, 
так что $z = (v, 0)$, где $v$ -- угловая точка множества $U$.

\section{Квадратичное программирование}

\begin{SmartDefinition}
 \textbf{Задачей квадратичного программирования} называется следующая задача
 оптимизации:
 \begin{equation}
  \begin{aligned}
  \label{eq:qp}
  J(u) = \frac{1}{2} \langle C u, u \rangle + \langle c, u \rangle \to min \\
  u \in U = \{u -in \mathbb{R}^{n} | \;\; \langle a_{i}, u \rangle \leq b^{i},
  i = 1, \ldots, m; \langle a_{i}, u \rangle = b^{i}, i = m + 1, \ldots, s\} \\
  \end{aligned}
 \end{equation}
 где $C$ -- симметричная неотрицательно определенная матрица размера
 $n \times n$, т. е.
 $C \geq 0$; $c, a_{i} \in \mathbb{R}^{n}, b^{i} \in \mathbb{R}^{n} (i = 1, 
 \ldots, s)$. При этом возможно что возможны такие частные случаи, когда могут
 быть только условия типа равенств ($m = 0$) или только условия типа неравенств
 ($s = m$) или никаких условий ($s = m = 0$).
\end{SmartDefinition}

Оказывается, для задачи (\ref{eq:qp}), как и для задачи линейного
программирования, существуют конечные (конечношаговые) методы их решения.

Рассмотрим функцию Лагранжа задачи (\ref{eq:qp}):

\begin{equation}
 \begin{aligned}
 L(u, \lambda) = \frac{1}{2} \langle C u, u \rangle + \langle c, u \rangle +
 \langle \lambda A u - b \rangle = \\
 \frac{1}{2} \langle C u, u \rangle +
 \langle c + A^{T} \lambda, u \rangle - \langle \lambda, b \rangle \\
 u \in U_{0} = \mathbb{R}^{n} \\
 \lambda \in \Lambda_{0} = \{\lambda = (\lambda_{1}, \ldots, \lambda_{s})
 \in \mathbb{R}^{s} | \;\; \lambda_{1} \geq 0, \ldots, \lambda_{m} \geq 0 \}
 \end{aligned}
\end{equation}

\begin{SmartDefinition}
 Следующая задача оптимизации называется \textbf{задачей, двойственной к задаче}
 (\ref{eq:qp}):
 \begin{equation}
  \begin{aligned}
   \psi(\lambda) = \operatornamewithlimits{inf}_{u \in \mathbb{R}^{n}}
   L(u, \lambda) \to sup, \;\;\; \lambda \in \Lambda_{0}
  \end{aligned}
 \end{equation}
\end{SmartDefinition}

Относительно двойственных задач в методах оптимизации существует довольно
формализованная теория. Так, следующая теорема (см. теорему 4.9.4 
из \cite{Vasilev1980}) устанавливает связь между точками экстремумов
первичной и двойственной задач.

\begin{SmartTheorem}
 Рассмотрим следующую задачу минимизации:
 \begin{equation}
  \begin{aligned}
  J(u) \to min \\
  u \in U = \{u \in U_{0} | \;\;
  g_{i}(u) = \langle a_{i}, u \rangle  - b^{i} \leq 0, i = 1, \ldots, m; \\
  g_{i}(u) = \langle a_{i}, u \rangle  - b^{i} = 0, i = m + 1, \ldots, s \}
  \end{aligned}
 \end{equation}
 где $U_{0}$ представляет из себя следующее многогранное множество:
 \begin{equation}
  \begin{aligned}
   U_{0} = \{u \in \mathbb{R}^{n} | \;\;
   \langle d_{i}, u \rangle \leq f^{i}, i = 1, \ldots, p; \\
   \langle d_{i}, u \rangle = f^{i}, i = p + 1, \ldots, q \}
  \end{aligned}
 \end{equation}
 $a_{i}, d_{i} \in \mathbb{R}^{n}$ -- заданные векторы, $b^{i}, f^{i}$ --
 заданные числа.
 Пусть функция $J(u)$ выпукла на $U_{0}$, $J(u) \in C^{1}(U_{0})$.
 Пусть множество $U_{*}$ есть множество решений задачи оптимизации:
 \begin{equation}
  U^{*} = \{u \in U | \;\; J(u) = \operatornamewithlimits{inf}_{v \in U} J(v)
  = J_{*} > - \infty\}
 \end{equation}
 Пусть $U_{*} \neq \varnothing$.
 
 Тогда для каждой точки $u_{*} \in U_{*}$ необходимо существуют множители
 Лагранжа
 $\lambda^{*} = (\lambda_{1}^{*}, \ldots, \lambda_{s}^{*}) \in \Lambda_{0}$
 где
 \begin{equation}
  \Lambda_{0} = \{\lambda = (\lambda_{1}, \ldots, \lambda_{s})
  \in \mathbb{R}^{s} | \;\; \lambda_{1} \geq 0, \ldots, \lambda_{m} \geq 0 \}
 \end{equation}
 такие, что пара $(u_{*}, \lambda^{*})$ образует седловую точку функции Лагранжа
 на множестве $U_{0} \times \Lambda_{0}$.
\end{SmartTheorem}

В соответствии с этой теоремой двойственная задача также имеет решение, причем

\begin{equation}
 \psi(\lambda^{*} = J(u_{*}))
\end{equation}

В случае если $C > 0$ функция $\psi(\lambda)$ может быть выписана явно. При
фиксированном $\lambda$ минимум функции Лагранжа по $u$ однозначно определяется
из системы $C u + c + A^{T} \lambda = 0$, так что
$u(\lambda) = - C^{-1} (c + A^{T} \lambda)$. Поэтому

\begin{equation}
 \begin{aligned}
  \psi(\lambda) = L(u(\lambda, \lambda)) =
  - \frac{1}{2} \langle c + A^{T} \lambda, C^{-1} (c + A^{T} \lambda) \rangle -
  \langle \lambda, b \rangle = \\
  = - \frac{1}{2} \langle (A C^{-1} A^{T}) \lambda, \lambda) \rangle
  - \frac{1}{2} \langle C^{-1} c, c \rangle, \;\; \lambda \in \Lambda_{0}
 \end{aligned}
\end{equation}

Решения первичной и двойственной задач связываются следующим соотношением:

\begin{equation}
 u_{*} = - C^{-1} (c + A^{T} \lambda^{*})
\end{equation}


В случае, когда ограничения типа неравенств отсутствуют, т. е. когда множество
ограничений представляет из себя следующее

\begin{equation}
 U = \{u \in \mathbb{R}^{n} | \;\; \langle a_{i}, b \rangle = b^{i},
 i = 1, \ldots, s\}
\end{equation}

то в двойственной задаче множество ограничений становится равным $\Lambda_{0} = 
\mathbb{R}^{s}$, то есть двойственная задача становится задачей безусловной 
минимизации

\begin{equation}
 - \psi (\lambda) \to inf, \;\; \lambda \in \mathbb{R}^{s}
\end{equation}

Для того, чтобы решить эту задачу, воспользуемся следующей теоремой (см. 
теорему 4.2.3 из (\cite{Vasilev1980})):

\begin{SmartTheorem}
 Пусть $U$ -- выпуклое множество, $J(u) \in C^{1}(U)$ и пусть $U_{*}$ --
 множество точек минимума $J(u)$ на $U$. Тогда в любой точке $u_{*} \in U_{*}$
 необходимо выполняется неравенство
 \begin{equation}
 \label{eq:optimality-condition}
  \langle J'(u_{*}), u - u_{*} \rangle \geq 0 \;\;\; \forall u \in U
 \end{equation}
 а в случае $u_{*} \in int U$ неравенство (\ref{eq:optimality-condition})
 превращается в равенство
 \begin{equation}
  J'(u_{*}) = 0
 \end{equation}
 Если, кроме того, $J(u)$ выпукла на $U$, то условие
 (\ref{eq:optimality-condition}) является достаточным для того, чтобы
 $u_{*} \in U_{*}$
\end{SmartTheorem}

Следовательно, всякое решение двойственной задачи квадратичного программирования
может быть найдено как решение следующей системы линейных уравнений:

\begin{equation}
 - \psi'(\lambda^{*}) = A C^{-1} A^{T} \lambda^{*} + A C^{-1} c + b = 0
\end{equation}

Покажем теперь, как решать задачу квадратичного программирования, когда есть
ограничения типа неравенств. Далее будем предполагать, что $C > 0$. Далее будет
показано, что задача с ограничениями типа неравенств может быть сведена к 
решению конечного числа задач с ограничениями типа равенств.

\begin{SmartDefinition}
 Точка $v$ называется \textbf{особой точкой задачи квадратичного 
 программирования} (\ref{eq:qp}), если $v \in U$ и $v$ является решением задачи
 \begin{equation}
 \label{eq:spec-point}
  \begin{aligned}
    J(u) \to inf \\
    u \in V = \{u \in \mathbb{R}^{n} | \;\; \langle a_{i}, u \rangle = b^{i},
    i \in I \cup \{m + 1, \ldots, s\}\}
  \end{aligned}
 \end{equation}
 где $I$ -- какое-либо подмножество индексов $\{1, \ldots, m\}$ (возможность
 $I = \varnothing$ не исключается).
\end{SmartDefinition}

С помощью несложных рассуждений можно доказать следующую лемму:

\begin{SmartLemma}
 Пусть в задаче (\ref{eq:qp}) $C \geq 0$, $J_{*} > - \infty$,
 $U_{*} \neq \varnothing$. Тогда каждое решение $u_{*}$ задачи квадратичного 
 программирования является особой  точкой этой задачи.
\end{SmartLemma}

Перебирая все возможные задачи с ограничениями типа равенств, соответствующие
исходной задаче, можно таким образом найти решение исходной задачи, то есть
справедлива следующая теорема:

\begin{SmartTheorem}
 Пусть $C > 0$ и множество $U$ в задаче квадратичного программирования непусто.
 Тогда существует конечный метод решения задачи (\ref{eq:qp}).
\end{SmartTheorem}

С точки зрения практики, однако, данная теорема не предоставляет никакой пользы,
поскольку перебор всех возможных особых точек слишком трудоемок для реальных
приложений. Поэтому используется специальная методика для нахождения решения
задачи квадратичного программирования. Она состоит из трех этапов.

\textbf{Этап 1.} Определяется, будет ли множество ограничений $U$ непустым, и 
если $U \neq \varnothing$, то находится какая-либо $v \in U$. Здесь может быть
использован, например, симплекс-метод, описанный в предыдущем разделе.

\textbf{Этап 2.} По известной неособой точке $v \in U$ строится особая точка
$w \in U$ со значением $J(w) \leq J(v)$. Для этого используется следующий
итерационный процесс:

\textbf{1.} $u_{0} = v$

\textbf{2.} Пусть известно $u_{k}$. Построим $\overline{u}_{k}$ как решение
задачи (\ref{eq:spec-point}) при

\begin{equation}
 I = I(u_{k}) = \{i: \;\;
 1 \leq i \leq m, \langle a_{i}, u_{k} \rangle = b^{i}\}
\end{equation}

\textbf{3.} Если $\overline{u}_{k} \in U$, то процесс останавливается и полагают
$w = \overline{u}_{k}$.

\textbf{4.} Если $\overline{u}_{k} \notin U$, то полагают

\begin{equation}
 I_{1} = \{j: \;\; \langle a_{j}, \overline{u}_{k} \rangle > b^{i},
 j \notin I(u_{k}), 1 \leq j \leq m\} \neq \varnothing
\end{equation}

\begin{equation}
 \begin{aligned}
  u_{k + 1} = u_{k} + \alpha_{k} (\overline{u}_{k} - u_{k}), \\
  \alpha_{k} = \operatornamewithlimits{min}_{j \in I_{1}}
  (\frac{b^{j} - \langle a_{j}, u_{k} \rangle}
  {\langle a_{j}, \overline{u}_{k} - u_{k \rangle}})
 \end{aligned}
\end{equation}

При этом можно показать что $u_{k + 1} \in U$. После этого мы возвращаемся к
пункту 2.

\textbf{Этап 3.} 

\textbf{1.} Выясняется, будет ли особая точка $w$, построенная на 2-м
этапе, решением задачи (\ref{eq:qp}).

\textbf{2.} Если это не так, то производится переход
к следующей точке $z \in U$, для которой $J(z) < J(w)$. Для этих целей 
достаточно совершить один шаг модифицированного метода условного градиента,
приняв в качестве начальной точку $w$, полученную на 2-м этапе. А именно,
сначала можно решить следующую задачу линейного программирования:

\begin{equation}
 \begin{aligned}
 \langle J'(w), e \rangle = \langle C w + c, e \rangle \to inf \\
 e \in \mathfrak{E} = \{e = (e^{1}, \ldots, e^{n}) \in \mathbb{R}^{n} | \;\;
 \langle a_{i}, e \rangle \leq 0, i \in I(w) = \{i: 1 \leq i \leq m,
 \langle a_{i}, w \rangle = b^{i}\} \\
 \langle a_{i}, e \rangle = 0, i = m + 1, \ldots, s, -1 \leq e^{j} \leq 1,
 j = 1, \ldots, n\}
 \end{aligned}
\end{equation}

\textbf{3.} Положим $\beta = \langle J'(w), e_{*} \rangle$. Очевидно, что
$\beta \leq 0$, поскольку нулевой вектор $0$ содержится в множестве
ограничений $\mathfrak{E}$ линейной задачи.

\textbf{4.} Если $\beta = 0$, то можно показать, что $w$ -- решение задачи
(\ref{eq:qp}).

\textbf{5.} Если же $\beta < 0$, то можно показать, что $e_{*}$ -- возможное
направление убывания функции $J(u)$ в точке $w$. Тогда полагают
$z = w + \alpha_{0} e_{*}$, где $\alpha_{0}$ -- достаточно малое число. И
производится возвращение к этапу 2.

Такое поочередное следование по этапам 2 и 3 алгоритма позволит 
построить последовательность особых точек исходной задачи, на которой 
функционал убывает и, следовательно, таким образом можно найти искомое решение
задачи.





\section{Рассматриваемые задачи}
\label{sec:problem}

В предыдущем отчете было указано, что задача восстановления многогранника по
набору его теневых контуров сводится в том или ином виде к задаче квадратичной
минимизации. В рассматриваемом случае функционал представлял из себя сумму
квадратов разностей опорных чисел и их экспериментальных измерений:

\begin{equation}
\label{eq:problem1}
 I(h) = I(h_{1}, h_{2}, \ldots, h_{n}) = ||h - h^{0}||_{2}^{2} =
 \sum \limits_{i = 1}^{n} (h_{i} - h_{i}^{0})^{2} \to min
\end{equation}

где числа $h_{i}^{0}$ есть экспериментальные измерения величин $h_{i}$.

При этом требовалось, чтобы вектор опорных чисел удовлетворял условию

\begin{equation}
\label{eq:problem1-cond}
 Q h \geq 0
\end{equation}

где матрица $Q$ представляла собой матрицу коэффициентов линейных неравенств
следующего типа:

\begin{equation}
\left|\begin{array}{cc}
  h_{1} & u_{1}^{T} \\
  h_{2} & u_{2}^{T} \\
  h_{3} & u_{3}^{T} \\
  h_{4} & u_{4}^{T} \\
\end{array}\right|
  \left|\begin{array}{cc}
  1 & u_{1}^{T} \\
  1 & u_{2}^{T} \\
  1 & u_{3}^{T} \\
  1 & u_{4}^{T} \\
\end{array}\right|
\geq 0
\end{equation}

где $u_{i}^{T}$ -- известные опорные направления, а $h_{i}$ -- неизвестные
опорные числа. Таким образом, матрица $Q$ представляет собой $m \times
n$-матрицу, причем в каждой ее строке содержится не более 4 ненулевых элементов,
и число $m$ ее строк не превышает $12 n$ (в соответствии с теоремой Эйлера о
числе вершин, ребер и граней графа на сфере).

Задачу минимизации (\ref{eq:problem1}) - (\ref{eq:problem1-cond}) можно
сформулировать несколько более удобным образом, как это описано, например, в
книге \cite{BertsekasTsitsiklis1989}: оптимальное решение задачи
(\ref{eq:problem1}) можно найти как

\begin{equation}
\label{eq:problem2}
 h^{*} = h^{0} - Q^{T} u^{*}
\end{equation}

где $u^{*}$ есть решение задачи минимизации функционала

\begin{equation}
\label{eq:problem2-cond}
 I(u_{1}, u_{2}, \ldots, u_{m}) = ||Q^{T} u - h^{0}||_{2}^{2} \to min
\end{equation}

при условиях неотрицательности переменных:

\begin{equation}
 u \geq 0
\end{equation}

Данный отчет посвящен поиску методов, которые позволили бы решить задачи
(\ref{eq:problem1}) - (\ref{eq:problem1-cond}) или (\ref{eq:problem2}) -
(\ref{eq:problem2-cond}) наиболее эффективно.

\section{Метод нормальных уравнений}

Если говорить о задачах наименьших квадратов вообще, то их можно выделить в
особый тип задач минимизации. Предположим, что матрица $A \in \mathbb{R}^{m
\times n}$ и $m > n$. В общем случае невозможно точно решить уравнение
$A x = b$, поскольку число условий больше числа неизвестных, лучшее что можно
сделать -- это минимизировать невязку $r = b - A x$. Задачи наименьших
квадратов формулируются как задачи минимизации евклидовой нормы невязки:

$$
||r||^{2}_{2} = \langle r, r \rangle \to min
$$

Один из способов решить задачу наименьших квадратов -- применить к ней прямой
и стандартный подход. Поскольку известно что $||r||^{2} = ||A x - b||^{2}$, то
производная по направлению $\partial x$ равна

$$
\nabla_{x} ||r||^{2} \cdot \partial x = 2 \langle A \partial x, b - A x\rangle =
2 \partial x^{T} (A^{T} b - A^{T} A x)
$$

Минимум

\section{<Временный раздел>}

Источники: \cite{BertsekasTsitsiklis1989, BierlaireTointTuyttens1991,
CantarellaPiatek2004, ChenDonohoSaunders2001, Golub1965, Lanczos1950,
PaigeSaunders1982, Saunders2013}.

\newpage
\bibliographystyle{plain}
\bibliography{references}

\end{document}
